{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac1d2bc",
   "metadata": {},
   "source": [
    "### Cap√≠tulo 16: Aprendizado por refor√ßo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428bc55a",
   "metadata": {},
   "source": [
    "O Cap√≠tulo 16, intitulado \"**Aprendizagem por Refor√ßo**\" (Reinforcement Learning - RL), apresenta este campo do Machine Learning como um dos mais empolgantes e antigos.\n",
    "\n",
    "### Introdu√ß√£o √† Aprendizagem por Refor√ßo (RL)\n",
    "\n",
    "A **RL** visa criar **agentes** capazes de agir em um **ambiente** para maximizar **recompensas** ao longo do tempo. O campo existe desde a d√©cada de 1950, mas ganhou destaque em 2013, quando pesquisadores da DeepMind demonstraram um sistema que podia aprender a jogar praticamente qualquer jogo de Atari usando apenas pixels brutos como entrada. Em mar√ßo de 2016, o sistema AlphaGo da DeepMind derrotou o campe√£o mundial de Go, Lee Sedol, marcando um avan√ßo significativo. A chave para esses sucessos foi a aplica√ß√£o do **Deep Learning** √† RL.\n",
    "\n",
    "### Conceitos Fundamentais da RL\n",
    "\n",
    "*   **Agente e Ambiente**\n",
    "    Um **agente** √© o sistema de software que observa o **ambiente**, escolhe e executa **a√ß√µes**, e recebe **recompensas** ou **penalidades**. O ambiente, por sua vez, recebe as a√ß√µes do agente e reage, produzindo novas observa√ß√µes e uma recompensa (ou sua aus√™ncia).\n",
    "\n",
    "*   **Observa√ß√µes, A√ß√µes e Recompensas**\n",
    "    O ambiente reage √†s a√ß√µes do agente, fornecendo novas **observa√ß√µes** e uma **recompensa** (positiva, negativa ou zero). O objetivo do agente √© aprender uma **pol√≠tica** que o ajude a selecionar a√ß√µes que maximizem as recompensas ao longo do tempo.\n",
    "\n",
    "*   **Aplica√ß√µes**\n",
    "    Exemplos incluem rob√¥s (que implementam algoritmos de **Aprendizagem por Refor√ßo** para aprender a andar), jogos (como Ms. Pac-Man e Go), termostatos, sistemas de trading autom√°tico, carros aut√¥nomos e posicionamento de an√∫ncios em p√°ginas da web.\n",
    "\n",
    "### Busca por Pol√≠ticas (Policy Search)\n",
    "\n",
    "A **pol√≠tica** √© o algoritmo que o agente usa para determinar suas a√ß√µes. Pode ser, por exemplo, uma rede neural que recebe observa√ß√µes como entrada e produz uma a√ß√£o.\n",
    "Existem diferentes abordagens para a busca de pol√≠ticas:\n",
    "*   **Algoritmos Gen√©ticos**: Mant√™m uma popula√ß√£o de pol√≠ticas, as avaliam, e permitem que as melhores se reproduzam e evoluam.\n",
    "*   **T√©cnicas de Otimiza√ß√£o**: Avaliam os gradientes das recompensas em rela√ß√£o aos par√¢metros da pol√≠tica e ajustam esses par√¢metros para buscar recompensas mais altas (ascens√£o de gradiente). Esta abordagem √© conhecida como **gradientes de pol√≠tica (policy gradients - PG)**.\n",
    "\n",
    "### OpenAI Gym\n",
    "\n",
    "√â um kit de ferramentas que fornece ambientes de simula√ß√£o para o desenvolvimento e compara√ß√£o de algoritmos de RL. Permite criar um ambiente (ex: `gym.make(\"CartPole-v0\")`), reinici√°-lo (`env.reset()`), renderiz√°-lo (`env.render()`), consultar as a√ß√µes poss√≠veis (`env.action_space`) e executar uma a√ß√£o (`env.step(action)`). O cap√≠tulo usa o ambiente **CartPole** como exemplo, onde um carrinho deve equilibrar um poste.\n",
    "\n",
    "### Pol√≠ticas de Rede Neural\n",
    "\n",
    "Uma pol√≠tica pode ser implementada como uma **rede neural** que recebe observa√ß√µes e produz a probabilidade de cada a√ß√£o poss√≠vel. Em vez de escolher a a√ß√£o com a maior probabilidade, muitas vezes √© escolhida uma a√ß√£o aleat√≥ria com base nessas probabilidades, permitindo um equil√≠brio entre **explora√ß√£o** (tentar novas a√ß√µes) e **explota√ß√£o** (usar a√ß√µes conhecidas por serem boas). Para o ambiente CartPole, que tem duas a√ß√µes, usa-se um neur√¥nio de sa√≠da com ativa√ß√£o log√≠stica; para mais a√ß√µes, usa-se softmax.\n",
    "\n",
    "### Avaliando A√ß√µes: O Problema de Atribui√ß√£o de Cr√©dito (Credit Assignment Problem)\n",
    "\n",
    "O problema surge porque as recompensas em RL s√£o frequentemente esparsas e atrasadas, dificultando saber quais a√ß√µes espec√≠ficas contribu√≠ram para um resultado. A solu√ß√£o comum √© avaliar uma a√ß√£o com base na soma de todas as recompensas que v√™m depois dela, aplicando uma **taxa de desconto** (`discount_rate`) a cada passo futuro. Uma taxa de desconto pr√≥xima de 0 valoriza recompensas imediatas, enquanto uma pr√≥xima de 1 valoriza recompensas futuras quase tanto quanto as imediatas. A√ß√µes com pontua√ß√£o negativa s√£o consideradas ruins, e as com pontua√ß√£o positiva s√£o consideradas boas.\n",
    "\n",
    "### Gradientes de Pol√≠tica (PG)\n",
    "\n",
    "O algoritmo PG treina a rede neural diretamente para produzir a√ß√µes que maximizam as recompensas. O processo geralmente envolve:\n",
    "1.  A rede neural calcula as probabilidades de a√ß√£o.\n",
    "2.  O agente executa epis√≥dios, coletando recompensas e gradientes.\n",
    "3.  As recompensas s√£o descontadas e normalizadas.\n",
    "4.  Os vetores de gradiente s√£o multiplicados pelas pontua√ß√µes das a√ß√µes (positivas para boas a√ß√µes, negativas para ruins).\n",
    "5.  √â dado um passo de **descida de gradiente** para ajustar os par√¢metros da rede, tornando as a√ß√µes bem-sucedidas mais prov√°veis no futuro.\n",
    "Este algoritmo, apesar de sua relativa simplicidade, √© bastante poderoso e foi a base do AlphaGo (juntamente com Monte Carlo Tree Search).\n",
    "\n",
    "### Processos de Decis√£o de Markov (MDPs)\n",
    "\n",
    "Um **MDP** √© um processo estoc√°stico (aleat√≥rio) sem mem√≥ria (assim como as **cadeias de Markov**), mas que inclui a tomada de decis√µes por um agente. √â definido por estados (`s`), a√ß√µes (`a`), recompensas (`R`) e probabilidades de transi√ß√£o (`T`) entre estados.\n",
    "*   **Valor √ìtimo do Estado** (`V*(s)`): A soma das recompensas futuras descontadas que um agente pode esperar em m√©dia, agindo otimamente a partir do estado `s`.\n",
    "*   **Equa√ß√£o de √ìtimalidade de Bellman**: Uma equa√ß√£o recursiva que define `V*(s)`.\n",
    "*   **Itera√ß√£o de Valor** (`Value Iteration`): Um algoritmo para estimar os valores √≥timos dos estados.\n",
    "*   **Q-Valores** (`Q*(s,a)`): O valor de uma a√ß√£o `a` tomada no estado `s`, seguida por a√ß√µes √≥timas a partir da√≠.\n",
    "*   **Itera√ß√£o de Valor-Q** (`Q-Value Iteration`): Um algoritmo para estimar os Q-Valores √≥timos. A **pol√≠tica √≥tima** (`œÄ*(s)`) √© ent√£o trivial: escolher a a√ß√£o com o Q-Valor mais alto para o estado atual.\n",
    "\n",
    "### Aprendizagem por Diferen√ßa Temporal (TD Learning)\n",
    "\n",
    "√â um algoritmo de RL que atualiza a estimativa do valor de um estado com base no valor do pr√≥ximo estado e na recompensa imediata, combinando ideias de Monte Carlo e programa√ß√£o din√¢mica. Ele se assemelha √† Descida de Gradiente Estoc√°stica (SGD).\n",
    "\n",
    "### Deep Q-Learning (DQN)\n",
    "\n",
    "Uma t√©cnica popular que utiliza **redes neurais profundas** para aproximar a fun√ß√£o Q-Value. A DeepMind a usou para jogar jogos de Atari, como Ms. Pac-Man.\n",
    "*   **Processamento de Observa√ß√µes**: Para jogos como Ms. Pac-Man, as observa√ß√µes (pixels da tela) s√£o pr√©-processadas (cortadas, redimensionadas, convertidas para tons de cinza, e t√™m o contraste melhorado).\n",
    "*   **Duas DQNs**: Geralmente, s√£o usadas duas DQNs com a mesma arquitetura, mas par√¢metros diferentes: uma atua como **ator** (guia o agente durante o treinamento) e a outra como **cr√≠tico** (aprende com as tentativas do ator). Periodicamente, os par√¢metros do cr√≠tico s√£o copiados para o ator.\n",
    "*   **Mem√≥ria de Replay** (`Replay Memory`): Uma t√©cnica crucial onde o agente armazena suas experi√™ncias passadas (observa√ß√£o, a√ß√£o, recompensa, pr√≥xima observa√ß√£o, se o jogo continua) e amostra aleatoriamente pequenos lotes para treinamento. Isso ajuda a lidar com a correla√ß√£o de observa√ß√µes consecutivas e a evitar o vi√©s no algoritmo de aprendizagem.\n",
    "*   **Pol√≠ticas de Explora√ß√£o**: Como a pol√≠tica **Œµ-greedy**, que escolhe uma a√ß√£o aleat√≥ria com probabilidade `Œµ` e a melhor a√ß√£o (de acordo com a DQN) com probabilidade `1-Œµ`.\n",
    "\n",
    "O cap√≠tulo conclui que o treinamento em RL pode ser lento e ruidoso, exigindo paci√™ncia e ajuste fino, mas os resultados finais s√£o muito empolgantes. As solu√ß√µes para os exerc√≠cios do cap√≠tulo est√£o dispon√≠veis nos notebooks Jupyter online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4140cd81",
   "metadata": {},
   "source": [
    "#### Implementa√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71581c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.spaces import Discrete\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3525ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Criar ambiente com modo de renderiza√ß√£o\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "# Resetar ambiente\n",
    "observation = env.reset()\n",
    "\n",
    "# Executar uma a√ß√£o aleat√≥ria\n",
    "action = env.action_space.sample()\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# Tentar renderizar\n",
    "try:\n",
    "    img = env.render()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Erro: {e}\")\n",
    "\n",
    "# Fechar ambiente\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e902edd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space\n",
    "Discrete(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095b5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "done = terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0062732f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04635585, -0.04583145, -0.03323687, -0.04035967], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a402477c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a4bf44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70a9b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d94255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M√©dia de recompensa: 41.76\n",
      "Desvio padr√£o: 9.02\n",
      "M√≠nimo: 24.0\n",
      "M√°ximo: 64.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')  # Sem render para ser mais r√°pido\n",
    "\n",
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs, info = env.reset()  # Gymnasium retorna (obs, info)\n",
    "    \n",
    "    for step in range(1000):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)  # 5 valores!\n",
    "        done = terminated or truncated\n",
    "        episode_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    totals.append(episode_rewards)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Estat√≠sticas dos resultados\n",
    "print(f\"M√©dia de recompensa: {np.mean(totals):.2f}\")\n",
    "print(f\"Desvio padr√£o: {np.std(totals):.2f}\")\n",
    "print(f\"M√≠nimo: {np.min(totals)}\")\n",
    "print(f\"M√°ximo: {np.max(totals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8509de",
   "metadata": {},
   "source": [
    "*Pol√≠ticas de Rede Neural*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "616dc8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A√ß√£o escolhida: 0 (0=esquerda, 1=direita)\n",
      "Probabilidade de ir para direita: 0.470\n"
     ]
    }
   ],
   "source": [
    "class PolicyNetwork(tf.keras.Model):\n",
    "    def __init__(self, n_inputs=4, n_hidden=4, n_outputs=1):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.hidden = tf.keras.layers.Dense(n_hidden, activation='elu',\n",
    "                                           kernel_initializer='he_normal')\n",
    "        self.logits = tf.keras.layers.Dense(n_outputs,\n",
    "                                           kernel_initializer='he_normal')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        hidden_out = self.hidden(inputs)\n",
    "        logits_out = self.logits(hidden_out)\n",
    "        return tf.nn.sigmoid(logits_out)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        # Adiciona dimens√£o de batch se necess√°rio\n",
    "        if len(state.shape) == 1:\n",
    "            state = tf.expand_dims(state, 0)\n",
    "        \n",
    "        # Calcula probabilidade de a√ß√£o direita\n",
    "        prob_right = self(state)\n",
    "        prob_left = 1 - prob_right\n",
    "        \n",
    "        # Concatena probabilidades\n",
    "        probabilities = tf.concat([prob_left, prob_right], axis=1)\n",
    "        \n",
    "        # Amostra a√ß√£o\n",
    "        action = tf.random.categorical(tf.math.log(probabilities), num_samples=1)\n",
    "        return action.numpy()[0, 0]  # Retorna a√ß√£o como escalar\n",
    "\n",
    "# Criar e usar a rede\n",
    "policy_net = PolicyNetwork()\n",
    "\n",
    "# Exemplo de uso\n",
    "sample_state = np.array([0.1, -0.2, 0.3, -0.4], dtype=np.float32)\n",
    "action = policy_net.select_action(sample_state)\n",
    "print(f\"A√ß√£o escolhida: {action} (0=esquerda, 1=direita)\")\n",
    "\n",
    "# Verificar probabilidades\n",
    "prob_right = policy_net(tf.expand_dims(sample_state, 0))\n",
    "print(f\"Probabilidade de ir para direita: {prob_right.numpy()[0, 0]:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f05f2",
   "metadata": {},
   "source": [
    "*Gradientes de pol√≠tica*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c4327fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = tf.constant([[0], [1], [0], [1]], dtype=tf.int32)\n",
    "y = 1. - tf.cast(action, tf.float32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0322\n"
     ]
    }
   ],
   "source": [
    "class SimplePGAgent:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(4, activation='elu'),\n",
    "            tf.keras.layers.Dense(1)  # Logits\n",
    "        ])\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = tf.expand_dims(state, 0) if len(state.shape) == 1 else state\n",
    "        logits = self.model(state)\n",
    "        prob = tf.nn.sigmoid(logits).numpy()[0, 0]\n",
    "        return 1 if np.random.random() < prob else 0, logits\n",
    "    \n",
    "    def train_step(self, states, actions, rewards, discount_rate=0.95):\n",
    "        # Converter para tensores\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        \n",
    "        # Calcular vantagens\n",
    "        advantages = self._compute_advantages(rewards, discount_rate)\n",
    "        advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            logits = self.model(states)\n",
    "            logits = tf.squeeze(logits, axis=-1)  # CORRE√á√ÉO: (batch, 1) ‚Üí (batch,)\n",
    "            \n",
    "            # Calcular loss\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=actions,  # shape (batch,)\n",
    "                    logits=logits    # shape (batch,)\n",
    "                ) * advantages\n",
    "            )\n",
    "        \n",
    "        # Backward pass\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "        return loss.numpy()\n",
    "    \n",
    "    def _compute_advantages(self, rewards, discount_rate):\n",
    "        discounted = np.zeros_like(rewards, dtype=np.float32)\n",
    "        cumulative = 0.0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            cumulative = rewards[i] + discount_rate * cumulative\n",
    "            discounted[i] = cumulative\n",
    "        return (discounted - np.mean(discounted)) / (np.std(discounted) + 1e-7)\n",
    "\n",
    "# Exemplo de uso r√°pido\n",
    "def test_agent():\n",
    "    agent = SimplePGAgent()\n",
    "    \n",
    "    # Dados de exemplo com shapes corretos\n",
    "    states = np.random.randn(10, 4)  # (batch, features)\n",
    "    actions = np.random.randint(0, 2, 10).astype(np.float32)  # (batch,)\n",
    "    rewards = np.random.randn(10)  # (batch,)\n",
    "    \n",
    "    # Testar o train_step\n",
    "    loss = agent.train_step(states, actions, rewards)\n",
    "    print(f\"Loss: {loss:.4f}\")  # Deve funcionar sem erro de shape\n",
    "\n",
    "test_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830bf3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensas originais: [10, 0, -50]\n",
      "Recompensas descontadas: [-22. -40. -50.]\n"
     ]
    }
   ],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    \"\"\"\n",
    "    Calcula recompensas descontadas (retornos).\n",
    "    \n",
    "    Args:\n",
    "        rewards: Lista de recompensas [r0, r1, r2, ..., rT]\n",
    "        discount_rate: Taxa de desconto (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        discounted_rewards: [G0, G1, G2, ..., GT]\n",
    "    \"\"\"\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    \n",
    "    # Percorre de tr√°s para frente\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + discount_rate * cumulative_rewards\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    \n",
    "    return discounted_rewards\n",
    "\n",
    "# Teste\n",
    "rewards = [10, 0, -50]\n",
    "discounted = discount_rewards(rewards, discount_rate=0.8)\n",
    "print(f\"Recompensas originais: {rewards}\")\n",
    "print(f\"Recompensas descontadas: {discounted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73af395b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Avg Reward = 32.40, Loss = -0.0340\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_0.keras\n",
      "Iteration 10: Avg Reward = 25.40, Loss = 0.0027\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_10.keras\n",
      "Iteration 20: Avg Reward = 42.40, Loss = 0.0048\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_20.keras\n",
      "Iteration 30: Avg Reward = 41.80, Loss = -0.0225\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_30.keras\n",
      "Iteration 40: Avg Reward = 37.00, Loss = -0.0057\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_40.keras\n",
      "Iteration 50: Avg Reward = 47.30, Loss = 0.0122\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_50.keras\n",
      "Iteration 60: Avg Reward = 65.80, Loss = -0.0110\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_60.keras\n",
      "Iteration 70: Avg Reward = 78.60, Loss = -0.0142\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_70.keras\n",
      "Iteration 80: Avg Reward = 105.50, Loss = 0.0022\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_80.keras\n",
      "Iteration 90: Avg Reward = 89.40, Loss = -0.0076\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_90.keras\n",
      "üéâ Treinamento conclu√≠do!\n"
     ]
    }
   ],
   "source": [
    "# Hiperpar√¢metros\n",
    "n_iterations = 100\n",
    "n_max_steps = 1000\n",
    "n_games_per_update = 10\n",
    "save_iterations = 10\n",
    "discount_rate = 0.95\n",
    "\n",
    "# Criar ambiente\n",
    "env = gym.make('CartPole-v1')\n",
    "n_inputs = env.observation_space.shape[0]\n",
    "\n",
    "# Rede neural\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(1, kernel_initializer='he_normal')  # Logits\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Fun√ß√µes auxiliares\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = []\n",
    "    for rewards in all_rewards:\n",
    "        discounted = np.zeros_like(rewards, dtype=np.float32)\n",
    "        cumulative = 0.0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            cumulative = rewards[i] + discount_rate * cumulative\n",
    "            discounted[i] = cumulative\n",
    "        all_discounted_rewards.append(discounted)\n",
    "    \n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    \n",
    "    return [(rewards - reward_mean) / (reward_std + 1e-7) \n",
    "            for rewards in all_discounted_rewards]\n",
    "\n",
    "def compute_loss(states, actions, advantages):\n",
    "    logits = model(states)\n",
    "    logits = tf.squeeze(logits, axis=-1)\n",
    "    \n",
    "    # Policy gradient loss\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(actions, tf.float32),\n",
    "            logits=logits\n",
    "        ) * advantages\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "# Diret√≥rio para salvar modelos (formato moderno .keras)\n",
    "checkpoint_dir = Path(\"./policy_net_checkpoints\")\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Loop de treinamento\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards = []\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "    \n",
    "    # Coletar dados de v√°rios epis√≥dios\n",
    "    for game in range(n_games_per_update):\n",
    "        states, actions, rewards = [], [], []\n",
    "        \n",
    "        obs, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            # Predizer a√ß√£o\n",
    "            state_tensor = tf.expand_dims(tf.convert_to_tensor(obs, dtype=tf.float32), 0)\n",
    "            logits = model(state_tensor)\n",
    "            prob_right = tf.nn.sigmoid(logits)\n",
    "            action_prob = prob_right.numpy()[0, 0]\n",
    "            action = 1 if np.random.random() < action_prob else 0\n",
    "            \n",
    "            # Executar a√ß√£o\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            # Armazenar dados\n",
    "            states.append(obs)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            obs = next_obs\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        all_rewards.append(rewards)\n",
    "        all_states.append(states)\n",
    "        all_actions.append(actions)\n",
    "    \n",
    "    # Calcular vantagens\n",
    "    all_advantages = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
    "    \n",
    "    # Preparar dados\n",
    "    all_states_flat = np.concatenate(all_states)\n",
    "    all_actions_flat = np.concatenate(all_actions)\n",
    "    all_advantages_flat = np.concatenate(all_advantages)\n",
    "    \n",
    "    # Treinamento\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(all_states_flat, all_actions_flat, all_advantages_flat)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    # Log e save\n",
    "    if iteration % save_iterations == 0:\n",
    "        avg_reward = np.mean([sum(rewards) for rewards in all_rewards])\n",
    "        print(f\"Iteration {iteration}: Avg Reward = {avg_reward:.2f}, Loss = {loss:.4f}\")\n",
    "        \n",
    "        # Salvar modelo no formato moderno (.keras)\n",
    "        model_path = checkpoint_dir / f\"policy_net_iter_{iteration}.keras\"\n",
    "        model.save(model_path)\n",
    "        print(f\"Model saved as {model_path}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"üéâ Treinamento conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01937fc8",
   "metadata": {},
   "source": [
    "Aprendizado de diferen√ßas temporais e Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b43bfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Learning rate = 0.0500\n",
      "Iteration 1000: Learning rate = 0.0005\n",
      "Iteration 2000: Learning rate = 0.0002\n",
      "Iteration 3000: Learning rate = 0.0002\n",
      "Iteration 4000: Learning rate = 0.0001\n",
      "Iteration 5000: Learning rate = 0.0001\n",
      "Iteration 6000: Learning rate = 0.0001\n",
      "Iteration 7000: Learning rate = 0.0001\n",
      "Iteration 8000: Learning rate = 0.0001\n",
      "Iteration 9000: Learning rate = 0.0001\n",
      "Iteration 10000: Learning rate = 0.0000\n",
      "Iteration 11000: Learning rate = 0.0000\n",
      "Iteration 12000: Learning rate = 0.0000\n",
      "Iteration 13000: Learning rate = 0.0000\n",
      "Iteration 14000: Learning rate = 0.0000\n",
      "Iteration 15000: Learning rate = 0.0000\n",
      "Iteration 16000: Learning rate = 0.0000\n",
      "Iteration 17000: Learning rate = 0.0000\n",
      "Iteration 18000: Learning rate = 0.0000\n",
      "Iteration 19000: Learning rate = 0.0000\n",
      "\n",
      "üéØ Q-table final:\n",
      "Estado | A√ß√£o 0 | A√ß√£o 1 | A√ß√£o 2\n",
      "-----------------------------------\n",
      "     0 |  0.397 |  0.042 |  0.019\n",
      "     1 | -0.026 | -0.003 |  0.300\n",
      "     2 | -0.042 |  0.293 | -0.043\n",
      "\n",
      "üß† Pol√≠tica √≥tima:\n",
      "Estado 0: Melhor a√ß√£o = 0, Valor = 0.397\n",
      "Estado 1: Melhor a√ß√£o = 2, Valor = 0.300\n",
      "Estado 2: Melhor a√ß√£o = 1, Valor = 0.293\n"
     ]
    }
   ],
   "source": [
    "# Par√¢metros do MDP\n",
    "n_states = 3\n",
    "n_actions = 3\n",
    "discount_rate = 0.95\n",
    "\n",
    "# Learning parameters\n",
    "learning_rate0 = 0.05  # Taxa de aprendizado inicial\n",
    "learning_rate_decay = 0.1\n",
    "n_iterations = 20000\n",
    "\n",
    "# Definir possible_actions (a√ß√µes poss√≠veis por estado)\n",
    "# Exemplo: cada estado tem todas as a√ß√µes dispon√≠veis\n",
    "possible_actions = [list(range(n_actions)) for _ in range(n_states)]\n",
    "\n",
    "# Matriz de transi√ß√µes T[s, a, s'] = P(s'|s, a)\n",
    "# Exemplo: transi√ß√µes aleat√≥rias para demonstra√ß√£o\n",
    "T = np.random.dirichlet(np.ones(n_states), size=(n_states, n_actions))\n",
    "\n",
    "# Matriz de recompensas R[s, a, s'] = recompensa\n",
    "# Exemplo: recompensas aleat√≥rias entre -1 e 1\n",
    "R = np.random.uniform(-1, 1, size=(n_states, n_actions, n_states))\n",
    "\n",
    "# Inicializar Q-table com -inf para a√ß√µes n√£o poss√≠veis\n",
    "Q = np.full((n_states, n_actions), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0  # Inicializar a√ß√µes poss√≠veis com 0\n",
    "\n",
    "# Q-Learning\n",
    "s = 0  # Estado inicial\n",
    "history = []  # Para acompanhar a converg√™ncia\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # Escolher a√ß√£o Œµ-greedy (explora√ß√£o vs explora√ß√£o)\n",
    "    if np.random.random() < 0.1:  # 10% exploration\n",
    "        a = np.random.choice(possible_actions[s])\n",
    "    else:  # 90% exploitation\n",
    "        a = np.argmax(Q[s])\n",
    "    \n",
    "    # Simular transi√ß√£o de estado\n",
    "    sp = np.random.choice(range(n_states), p=T[s, a])\n",
    "    reward = R[s, a, sp]\n",
    "    \n",
    "    # Decaimento da taxa de aprendizado\n",
    "    learning_rate = learning_rate0 / (1 + iteration * learning_rate_decay)\n",
    "    \n",
    "    # Atualiza√ß√£o Q-learning\n",
    "    Q[s, a] = ((1 - learning_rate) * Q[s, a] + \n",
    "               learning_rate * (reward + discount_rate * np.max(Q[sp])))\n",
    "    \n",
    "    # Registrar para an√°lise\n",
    "    if iteration % 1000 == 0:\n",
    "        history.append(Q.copy())\n",
    "        print(f\"Iteration {iteration}: Learning rate = {learning_rate:.4f}\")\n",
    "    \n",
    "    # Mudar para pr√≥ximo estado\n",
    "    s = sp\n",
    "\n",
    "# Resultados finais\n",
    "print(\"\\nüéØ Q-table final:\")\n",
    "print(\"Estado | A√ß√£o 0 | A√ß√£o 1 | A√ß√£o 2\")\n",
    "print(\"-\" * 35)\n",
    "for state in range(n_states):\n",
    "    print(f\"{state:6} | {Q[state, 0]:6.3f} | {Q[state, 1]:6.3f} | {Q[state, 2]:6.3f}\")\n",
    "\n",
    "print(\"\\nüß† Pol√≠tica √≥tima:\")\n",
    "for state in range(n_states):\n",
    "    best_action = np.argmax(Q[state])\n",
    "    print(f\"Estado {state}: Melhor a√ß√£o = {best_action}, Valor = {Q[state, best_action]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f89b5",
   "metadata": {},
   "source": [
    "#### Exerc√≠cios:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695fe4ce",
   "metadata": {},
   "source": [
    "1. Como voc√™ definiria o Aprendizado por refor√ßo? Como ele √© diferente do aprendizado regular supervisionado ou n√£o supervisionado?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641febef",
   "metadata": {},
   "source": [
    "**Aprendizado por Refor√ßo** √© um tipo de aprendizado de m√°quina onde um agente aprende a tomar decis√µes atrav√©s de tentativa e erro, recebendo recompensas ou puni√ß√µes como feedback. Diferente do aprendizado supervisionado (que usa exemplos pr√©-definidos com respostas corretas) e do n√£o supervisionado (que busca padr√µes sem feedback), o aprendizado por refor√ßo aprende interagindo com o ambiente e maximizando recompensas acumuladas ao longo do tempo.\n",
    "\n",
    "Enquanto o aprendizado supervisionado precisa de um \"professor\" fornecendo respostas exatas, e o n√£o supervisionado descobre estrutura sozinho, o aprendizado por refor√ßo aprende pela experi√™ncia, como um jogador que melhora praticando e recebendo pontos por suas a√ß√µes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf6586",
   "metadata": {},
   "source": [
    "2. Voc√™ consegue pensar em tr√™s poss√≠veis aplica√ß√µes do RL que n√£o foram mencionadas neste cap√≠tulo? Qual √© o ambiente para cada uma delas? Qual √© o agente? quais s√£o as a√ß√µes poss√≠veis? Quais s√£o as recompensas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70adce",
   "metadata": {},
   "source": [
    "**A. Otimiza√ß√£o de Tr√°fego em Tempo Real**\n",
    "- **Ambiente**: Rede de sem√°foros de uma cidade\n",
    "- **Agente**: Sistema de controle de tr√°fego\n",
    "- **A√ß√µes**: Alterar tempos dos sem√°foros, priorizar vias espec√≠ficas\n",
    "- **Recompensas**: Redu√ß√£o do tempo m√©dio de espera, aumento do fluxo de ve√≠culos\n",
    "\n",
    "**B. Gest√£o Automatizada de Portf√≥lio Financeiro**\n",
    "- **Ambiente**: Mercado financeiro com a√ß√µes, t√≠tulos e commodities\n",
    "- **Agente**: Sistema de investimentos automatizado\n",
    "- **A√ß√µes**: Comprar, vender ou manter ativos financeiros\n",
    "- **Recompensas**: Maximizar retorno do portf√≥lio, minimizar riscos\n",
    "\n",
    "**C. Controle de Efici√™ncia Energ√©tica em Data Centers**\n",
    "- **Ambiente**: Sistema de refrigera√ß√£o e servidores do data center\n",
    "- **Agente**: Controlador de gest√£o energ√©tica\n",
    "- **A√ß√µes**: Ajustar temperaturas, redistribuir cargas, gerenciar cooling\n",
    "- **Recompensas**: Redu√ß√£o do consumo energ√©tico, manuten√ß√£o da temperatura ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d581769",
   "metadata": {},
   "source": [
    "3. Qual √© a taxa de desconto? A pol√≠tica √≥tima pode mudar se voc√™ modificar a taxa de desconto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9365a8e",
   "metadata": {},
   "source": [
    "A **taxa de desconto** (Œ≥) √© um hiperpar√¢metro entre 0 e 1 que determina o valor presente de recompensas futuras. Valores pr√≥ximos a 1 tornam o agente mais orientado a longo prazo, enquanto valores pr√≥ximos a 0 focam em recompensas imediatas.\n",
    "\n",
    "Sim, a **pol√≠tica √≥tima pode mudar** ao alterar a taxa de desconto. Uma taxa baixa prioriza recompensas imediatas, levando a pol√≠ticas mais \"curtas\" e oportunistas. J√° uma taxa alta valoriza consequ√™ncias futuras, resultando em pol√≠ticas mais estrat√©gicas e de longo prazo. A escolha da taxa reflete o trade-off entre satisfa√ß√£o imediata e planejamento futuro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c317a",
   "metadata": {},
   "source": [
    "4. Como voc√™ mede o desempenho de um agente do aprendizado por refor√ßo?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026ce8e0",
   "metadata": {},
   "source": [
    "O desempenho de um agente de aprendizado por refor√ßo √© medido principalmente pela **soma acumulada de recompensas** obtida durante os epis√≥dios de treinamento e teste. Outras m√©tricas importantes incluem a **taxa de sucesso** (quantas vezes atingiu o objetivo), o **tempo de converg√™ncia** (quantos epis√≥dios at√© estabilizar o desempenho) e a **consist√™ncia** (varia√ß√£o do desempenho entre execu√ß√µes). Em ambientes cont√≠nuos, tamb√©m se avalia a **efici√™ncia** (custo computacional por a√ß√£o) e a **capacidade de generaliza√ß√£o** para situa√ß√µes n√£o vistas durante o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79673e1",
   "metadata": {},
   "source": [
    "5. Qual √© o problema da atribui√ß√£o de cr√©dito? Quando ocorre? Como pode ser aliviado?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c612f2",
   "metadata": {},
   "source": [
    "O **problema da atribui√ß√£o de cr√©dito** ocorre quando um agente recebe uma recompensa ap√≥s uma sequ√™ncia de a√ß√µes e precisa determinar quais a√ß√µes espec√≠ficas contribu√≠ram para esse resultado. Este problema √© comum em tarefas com recompensas esparsas ou atrasadas, onde o feedback n√£o √© imediato.\n",
    "\n",
    "Pode ser aliviado atrav√©s de m√©todos como **Discounting** (valorizando recompensas imediatas), **Eligibility Traces** (rastreando contribui√ß√µes de a√ß√µes recentes) e **Reward Shaping** (fornecendo recompensas intermedi√°rias artificiais para guiar o aprendizado). Algoritmos como **Actor-Critic** tamb√©m ajudam ao separar a avalia√ß√£o de valores da sele√ß√£o de a√ß√µes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df70696",
   "metadata": {},
   "source": [
    "6. Qual √© o objetivo de utilizar uma mem√≥ria de repeti√ß√£o?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b7ca6",
   "metadata": {},
   "source": [
    "A **mem√≥ria de repeti√ß√£o** (replay buffer) √© utilizada para armazenar experi√™ncias passadas do agente (estado, a√ß√£o, recompensa, pr√≥ximo estado), permitindo o reaproveitamento dessas experi√™ncias em treinamentos futuros. Seu objetivo principal √© **quebrar correla√ß√µes temporais** entre experi√™ncias sequenciais, aumentar a **efici√™ncia de amostragem** e **estabilizar o treinamento** ao permitir que o agente aprenda repetidamente com experi√™ncias diversificadas, incluindo tanto sucessos quanto fracassos passados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a23ea7",
   "metadata": {},
   "source": [
    "7. O que √© um algoritmo de RL off-policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f8f3dc",
   "metadata": {},
   "source": [
    "Um algoritmo **off-policy** √© um m√©todo de aprendizado por refor√ßo onde o agente aprende sobre uma pol√≠tica √≥tima (pol√≠tica alvo) enquanto segue uma pol√≠tica diferente de explora√ß√£o (pol√≠tica comportamental). Isso permite aprender com experi√™ncias passadas geradas por pol√≠ticas diferentes, reutilizar dados de forma mais eficiente e explorar o ambiente sem comprometer o aprendizado da pol√≠tica ideal. Exemplos comuns incluem Q-Learning e DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc5b88",
   "metadata": {},
   "source": [
    "8. Utilize gradientes de pol√≠ticas para enfrentar o \"BypedalWalker-v2\" da OpenAI gym."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52687414",
   "metadata": {},
   "source": [
    "9. Utilize o algoritmo DQN para treinar um agente para jogar Pong, o famoso jogo do atari(Pong-v0 no OpenAI gym). Cuidado: uma observa√ß√£o individual √© insuficiente para dizer a dire√ß√£o e a velocidade da bola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9718e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
