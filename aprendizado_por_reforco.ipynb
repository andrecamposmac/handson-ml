{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac1d2bc",
   "metadata": {},
   "source": [
    "### Capítulo 16: Aprendizado por reforço"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428bc55a",
   "metadata": {},
   "source": [
    "O Capítulo 16, intitulado \"**Aprendizagem por Reforço**\" (Reinforcement Learning - RL), apresenta este campo do Machine Learning como um dos mais empolgantes e antigos.\n",
    "\n",
    "### Introdução à Aprendizagem por Reforço (RL)\n",
    "\n",
    "A **RL** visa criar **agentes** capazes de agir em um **ambiente** para maximizar **recompensas** ao longo do tempo. O campo existe desde a década de 1950, mas ganhou destaque em 2013, quando pesquisadores da DeepMind demonstraram um sistema que podia aprender a jogar praticamente qualquer jogo de Atari usando apenas pixels brutos como entrada. Em março de 2016, o sistema AlphaGo da DeepMind derrotou o campeão mundial de Go, Lee Sedol, marcando um avanço significativo. A chave para esses sucessos foi a aplicação do **Deep Learning** à RL.\n",
    "\n",
    "### Conceitos Fundamentais da RL\n",
    "\n",
    "*   **Agente e Ambiente**\n",
    "    Um **agente** é o sistema de software que observa o **ambiente**, escolhe e executa **ações**, e recebe **recompensas** ou **penalidades**. O ambiente, por sua vez, recebe as ações do agente e reage, produzindo novas observações e uma recompensa (ou sua ausência).\n",
    "\n",
    "*   **Observações, Ações e Recompensas**\n",
    "    O ambiente reage às ações do agente, fornecendo novas **observações** e uma **recompensa** (positiva, negativa ou zero). O objetivo do agente é aprender uma **política** que o ajude a selecionar ações que maximizem as recompensas ao longo do tempo.\n",
    "\n",
    "*   **Aplicações**\n",
    "    Exemplos incluem robôs (que implementam algoritmos de **Aprendizagem por Reforço** para aprender a andar), jogos (como Ms. Pac-Man e Go), termostatos, sistemas de trading automático, carros autônomos e posicionamento de anúncios em páginas da web.\n",
    "\n",
    "### Busca por Políticas (Policy Search)\n",
    "\n",
    "A **política** é o algoritmo que o agente usa para determinar suas ações. Pode ser, por exemplo, uma rede neural que recebe observações como entrada e produz uma ação.\n",
    "Existem diferentes abordagens para a busca de políticas:\n",
    "*   **Algoritmos Genéticos**: Mantêm uma população de políticas, as avaliam, e permitem que as melhores se reproduzam e evoluam.\n",
    "*   **Técnicas de Otimização**: Avaliam os gradientes das recompensas em relação aos parâmetros da política e ajustam esses parâmetros para buscar recompensas mais altas (ascensão de gradiente). Esta abordagem é conhecida como **gradientes de política (policy gradients - PG)**.\n",
    "\n",
    "### OpenAI Gym\n",
    "\n",
    "É um kit de ferramentas que fornece ambientes de simulação para o desenvolvimento e comparação de algoritmos de RL. Permite criar um ambiente (ex: `gym.make(\"CartPole-v0\")`), reiniciá-lo (`env.reset()`), renderizá-lo (`env.render()`), consultar as ações possíveis (`env.action_space`) e executar uma ação (`env.step(action)`). O capítulo usa o ambiente **CartPole** como exemplo, onde um carrinho deve equilibrar um poste.\n",
    "\n",
    "### Políticas de Rede Neural\n",
    "\n",
    "Uma política pode ser implementada como uma **rede neural** que recebe observações e produz a probabilidade de cada ação possível. Em vez de escolher a ação com a maior probabilidade, muitas vezes é escolhida uma ação aleatória com base nessas probabilidades, permitindo um equilíbrio entre **exploração** (tentar novas ações) e **explotação** (usar ações conhecidas por serem boas). Para o ambiente CartPole, que tem duas ações, usa-se um neurônio de saída com ativação logística; para mais ações, usa-se softmax.\n",
    "\n",
    "### Avaliando Ações: O Problema de Atribuição de Crédito (Credit Assignment Problem)\n",
    "\n",
    "O problema surge porque as recompensas em RL são frequentemente esparsas e atrasadas, dificultando saber quais ações específicas contribuíram para um resultado. A solução comum é avaliar uma ação com base na soma de todas as recompensas que vêm depois dela, aplicando uma **taxa de desconto** (`discount_rate`) a cada passo futuro. Uma taxa de desconto próxima de 0 valoriza recompensas imediatas, enquanto uma próxima de 1 valoriza recompensas futuras quase tanto quanto as imediatas. Ações com pontuação negativa são consideradas ruins, e as com pontuação positiva são consideradas boas.\n",
    "\n",
    "### Gradientes de Política (PG)\n",
    "\n",
    "O algoritmo PG treina a rede neural diretamente para produzir ações que maximizam as recompensas. O processo geralmente envolve:\n",
    "1.  A rede neural calcula as probabilidades de ação.\n",
    "2.  O agente executa episódios, coletando recompensas e gradientes.\n",
    "3.  As recompensas são descontadas e normalizadas.\n",
    "4.  Os vetores de gradiente são multiplicados pelas pontuações das ações (positivas para boas ações, negativas para ruins).\n",
    "5.  É dado um passo de **descida de gradiente** para ajustar os parâmetros da rede, tornando as ações bem-sucedidas mais prováveis no futuro.\n",
    "Este algoritmo, apesar de sua relativa simplicidade, é bastante poderoso e foi a base do AlphaGo (juntamente com Monte Carlo Tree Search).\n",
    "\n",
    "### Processos de Decisão de Markov (MDPs)\n",
    "\n",
    "Um **MDP** é um processo estocástico (aleatório) sem memória (assim como as **cadeias de Markov**), mas que inclui a tomada de decisões por um agente. É definido por estados (`s`), ações (`a`), recompensas (`R`) e probabilidades de transição (`T`) entre estados.\n",
    "*   **Valor Ótimo do Estado** (`V*(s)`): A soma das recompensas futuras descontadas que um agente pode esperar em média, agindo otimamente a partir do estado `s`.\n",
    "*   **Equação de Ótimalidade de Bellman**: Uma equação recursiva que define `V*(s)`.\n",
    "*   **Iteração de Valor** (`Value Iteration`): Um algoritmo para estimar os valores ótimos dos estados.\n",
    "*   **Q-Valores** (`Q*(s,a)`): O valor de uma ação `a` tomada no estado `s`, seguida por ações ótimas a partir daí.\n",
    "*   **Iteração de Valor-Q** (`Q-Value Iteration`): Um algoritmo para estimar os Q-Valores ótimos. A **política ótima** (`π*(s)`) é então trivial: escolher a ação com o Q-Valor mais alto para o estado atual.\n",
    "\n",
    "### Aprendizagem por Diferença Temporal (TD Learning)\n",
    "\n",
    "É um algoritmo de RL que atualiza a estimativa do valor de um estado com base no valor do próximo estado e na recompensa imediata, combinando ideias de Monte Carlo e programação dinâmica. Ele se assemelha à Descida de Gradiente Estocástica (SGD).\n",
    "\n",
    "### Deep Q-Learning (DQN)\n",
    "\n",
    "Uma técnica popular que utiliza **redes neurais profundas** para aproximar a função Q-Value. A DeepMind a usou para jogar jogos de Atari, como Ms. Pac-Man.\n",
    "*   **Processamento de Observações**: Para jogos como Ms. Pac-Man, as observações (pixels da tela) são pré-processadas (cortadas, redimensionadas, convertidas para tons de cinza, e têm o contraste melhorado).\n",
    "*   **Duas DQNs**: Geralmente, são usadas duas DQNs com a mesma arquitetura, mas parâmetros diferentes: uma atua como **ator** (guia o agente durante o treinamento) e a outra como **crítico** (aprende com as tentativas do ator). Periodicamente, os parâmetros do crítico são copiados para o ator.\n",
    "*   **Memória de Replay** (`Replay Memory`): Uma técnica crucial onde o agente armazena suas experiências passadas (observação, ação, recompensa, próxima observação, se o jogo continua) e amostra aleatoriamente pequenos lotes para treinamento. Isso ajuda a lidar com a correlação de observações consecutivas e a evitar o viés no algoritmo de aprendizagem.\n",
    "*   **Políticas de Exploração**: Como a política **ε-greedy**, que escolhe uma ação aleatória com probabilidade `ε` e a melhor ação (de acordo com a DQN) com probabilidade `1-ε`.\n",
    "\n",
    "O capítulo conclui que o treinamento em RL pode ser lento e ruidoso, exigindo paciência e ajuste fino, mas os resultados finais são muito empolgantes. As soluções para os exercícios do capítulo estão disponíveis nos notebooks Jupyter online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4140cd81",
   "metadata": {},
   "source": [
    "#### Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71581c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from gymnasium.spaces import Discrete\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3525ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Criar ambiente com modo de renderização\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "# Resetar ambiente\n",
    "observation = env.reset()\n",
    "\n",
    "# Executar uma ação aleatória\n",
    "action = env.action_space.sample()\n",
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "# Tentar renderizar\n",
    "try:\n",
    "    img = env.render()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Erro: {e}\")\n",
    "\n",
    "# Fechar ambiente\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e902edd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space\n",
    "Discrete(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "095b5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 1\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "done = terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0062732f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04635585, -0.04583145, -0.03323687, -0.04035967], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a402477c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a4bf44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70a9b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d94255c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média de recompensa: 41.76\n",
      "Desvio padrão: 9.02\n",
      "Mínimo: 24.0\n",
      "Máximo: 64.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')  # Sem render para ser mais rápido\n",
    "\n",
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs, info = env.reset()  # Gymnasium retorna (obs, info)\n",
    "    \n",
    "    for step in range(1000):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)  # 5 valores!\n",
    "        done = terminated or truncated\n",
    "        episode_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    totals.append(episode_rewards)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Estatísticas dos resultados\n",
    "print(f\"Média de recompensa: {np.mean(totals):.2f}\")\n",
    "print(f\"Desvio padrão: {np.std(totals):.2f}\")\n",
    "print(f\"Mínimo: {np.min(totals)}\")\n",
    "print(f\"Máximo: {np.max(totals)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8509de",
   "metadata": {},
   "source": [
    "*Políticas de Rede Neural*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "616dc8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ação escolhida: 0 (0=esquerda, 1=direita)\n",
      "Probabilidade de ir para direita: 0.470\n"
     ]
    }
   ],
   "source": [
    "class PolicyNetwork(tf.keras.Model):\n",
    "    def __init__(self, n_inputs=4, n_hidden=4, n_outputs=1):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.hidden = tf.keras.layers.Dense(n_hidden, activation='elu',\n",
    "                                           kernel_initializer='he_normal')\n",
    "        self.logits = tf.keras.layers.Dense(n_outputs,\n",
    "                                           kernel_initializer='he_normal')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        hidden_out = self.hidden(inputs)\n",
    "        logits_out = self.logits(hidden_out)\n",
    "        return tf.nn.sigmoid(logits_out)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        # Adiciona dimensão de batch se necessário\n",
    "        if len(state.shape) == 1:\n",
    "            state = tf.expand_dims(state, 0)\n",
    "        \n",
    "        # Calcula probabilidade de ação direita\n",
    "        prob_right = self(state)\n",
    "        prob_left = 1 - prob_right\n",
    "        \n",
    "        # Concatena probabilidades\n",
    "        probabilities = tf.concat([prob_left, prob_right], axis=1)\n",
    "        \n",
    "        # Amostra ação\n",
    "        action = tf.random.categorical(tf.math.log(probabilities), num_samples=1)\n",
    "        return action.numpy()[0, 0]  # Retorna ação como escalar\n",
    "\n",
    "# Criar e usar a rede\n",
    "policy_net = PolicyNetwork()\n",
    "\n",
    "# Exemplo de uso\n",
    "sample_state = np.array([0.1, -0.2, 0.3, -0.4], dtype=np.float32)\n",
    "action = policy_net.select_action(sample_state)\n",
    "print(f\"Ação escolhida: {action} (0=esquerda, 1=direita)\")\n",
    "\n",
    "# Verificar probabilidades\n",
    "prob_right = policy_net(tf.expand_dims(sample_state, 0))\n",
    "print(f\"Probabilidade de ir para direita: {prob_right.numpy()[0, 0]:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023f05f2",
   "metadata": {},
   "source": [
    "*Gradientes de política*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c4327fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = tf.constant([[0], [1], [0], [1]], dtype=tf.int32)\n",
    "y = 1. - tf.cast(action, tf.float32) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0322\n"
     ]
    }
   ],
   "source": [
    "class SimplePGAgent:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(4, activation='elu'),\n",
    "            tf.keras.layers.Dense(1)  # Logits\n",
    "        ])\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = tf.expand_dims(state, 0) if len(state.shape) == 1 else state\n",
    "        logits = self.model(state)\n",
    "        prob = tf.nn.sigmoid(logits).numpy()[0, 0]\n",
    "        return 1 if np.random.random() < prob else 0, logits\n",
    "    \n",
    "    def train_step(self, states, actions, rewards, discount_rate=0.95):\n",
    "        # Converter para tensores\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        \n",
    "        # Calcular vantagens\n",
    "        advantages = self._compute_advantages(rewards, discount_rate)\n",
    "        advantages = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            logits = self.model(states)\n",
    "            logits = tf.squeeze(logits, axis=-1)  # CORREÇÃO: (batch, 1) → (batch,)\n",
    "            \n",
    "            # Calcular loss\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    labels=actions,  # shape (batch,)\n",
    "                    logits=logits    # shape (batch,)\n",
    "                ) * advantages\n",
    "            )\n",
    "        \n",
    "        # Backward pass\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "        return loss.numpy()\n",
    "    \n",
    "    def _compute_advantages(self, rewards, discount_rate):\n",
    "        discounted = np.zeros_like(rewards, dtype=np.float32)\n",
    "        cumulative = 0.0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            cumulative = rewards[i] + discount_rate * cumulative\n",
    "            discounted[i] = cumulative\n",
    "        return (discounted - np.mean(discounted)) / (np.std(discounted) + 1e-7)\n",
    "\n",
    "# Exemplo de uso rápido\n",
    "def test_agent():\n",
    "    agent = SimplePGAgent()\n",
    "    \n",
    "    # Dados de exemplo com shapes corretos\n",
    "    states = np.random.randn(10, 4)  # (batch, features)\n",
    "    actions = np.random.randint(0, 2, 10).astype(np.float32)  # (batch,)\n",
    "    rewards = np.random.randn(10)  # (batch,)\n",
    "    \n",
    "    # Testar o train_step\n",
    "    loss = agent.train_step(states, actions, rewards)\n",
    "    print(f\"Loss: {loss:.4f}\")  # Deve funcionar sem erro de shape\n",
    "\n",
    "test_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830bf3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recompensas originais: [10, 0, -50]\n",
      "Recompensas descontadas: [-22. -40. -50.]\n"
     ]
    }
   ],
   "source": [
    "def discount_rewards(rewards, discount_rate):\n",
    "    \"\"\"\n",
    "    Calcula recompensas descontadas (retornos).\n",
    "    \n",
    "    Args:\n",
    "        rewards: Lista de recompensas [r0, r1, r2, ..., rT]\n",
    "        discount_rate: Taxa de desconto (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        discounted_rewards: [G0, G1, G2, ..., GT]\n",
    "    \"\"\"\n",
    "    discounted_rewards = np.empty(len(rewards))\n",
    "    cumulative_rewards = 0\n",
    "    \n",
    "    # Percorre de trás para frente\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        cumulative_rewards = rewards[step] + discount_rate * cumulative_rewards\n",
    "        discounted_rewards[step] = cumulative_rewards\n",
    "    \n",
    "    return discounted_rewards\n",
    "\n",
    "# Teste\n",
    "rewards = [10, 0, -50]\n",
    "discounted = discount_rewards(rewards, discount_rate=0.8)\n",
    "print(f\"Recompensas originais: {rewards}\")\n",
    "print(f\"Recompensas descontadas: {discounted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73af395b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Avg Reward = 32.40, Loss = -0.0340\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_0.keras\n",
      "Iteration 10: Avg Reward = 25.40, Loss = 0.0027\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_10.keras\n",
      "Iteration 20: Avg Reward = 42.40, Loss = 0.0048\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_20.keras\n",
      "Iteration 30: Avg Reward = 41.80, Loss = -0.0225\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_30.keras\n",
      "Iteration 40: Avg Reward = 37.00, Loss = -0.0057\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_40.keras\n",
      "Iteration 50: Avg Reward = 47.30, Loss = 0.0122\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_50.keras\n",
      "Iteration 60: Avg Reward = 65.80, Loss = -0.0110\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_60.keras\n",
      "Iteration 70: Avg Reward = 78.60, Loss = -0.0142\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_70.keras\n",
      "Iteration 80: Avg Reward = 105.50, Loss = 0.0022\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_80.keras\n",
      "Iteration 90: Avg Reward = 89.40, Loss = -0.0076\n",
      "Model saved as policy_net_checkpoints\\policy_net_iter_90.keras\n",
      "🎉 Treinamento concluído!\n"
     ]
    }
   ],
   "source": [
    "# Hiperparâmetros\n",
    "n_iterations = 100\n",
    "n_max_steps = 1000\n",
    "n_games_per_update = 10\n",
    "save_iterations = 10\n",
    "discount_rate = 0.95\n",
    "\n",
    "# Criar ambiente\n",
    "env = gym.make('CartPole-v1')\n",
    "n_inputs = env.observation_space.shape[0]\n",
    "\n",
    "# Rede neural\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, activation='elu', kernel_initializer='he_normal'),\n",
    "    tf.keras.layers.Dense(1, kernel_initializer='he_normal')  # Logits\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Funções auxiliares\n",
    "def discount_and_normalize_rewards(all_rewards, discount_rate):\n",
    "    all_discounted_rewards = []\n",
    "    for rewards in all_rewards:\n",
    "        discounted = np.zeros_like(rewards, dtype=np.float32)\n",
    "        cumulative = 0.0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            cumulative = rewards[i] + discount_rate * cumulative\n",
    "            discounted[i] = cumulative\n",
    "        all_discounted_rewards.append(discounted)\n",
    "    \n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    \n",
    "    return [(rewards - reward_mean) / (reward_std + 1e-7) \n",
    "            for rewards in all_discounted_rewards]\n",
    "\n",
    "def compute_loss(states, actions, advantages):\n",
    "    logits = model(states)\n",
    "    logits = tf.squeeze(logits, axis=-1)\n",
    "    \n",
    "    # Policy gradient loss\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            labels=tf.cast(actions, tf.float32),\n",
    "            logits=logits\n",
    "        ) * advantages\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "# Diretório para salvar modelos (formato moderno .keras)\n",
    "checkpoint_dir = Path(\"./policy_net_checkpoints\")\n",
    "checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Loop de treinamento\n",
    "for iteration in range(n_iterations):\n",
    "    all_rewards = []\n",
    "    all_states = []\n",
    "    all_actions = []\n",
    "    \n",
    "    # Coletar dados de vários episódios\n",
    "    for game in range(n_games_per_update):\n",
    "        states, actions, rewards = [], [], []\n",
    "        \n",
    "        obs, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            # Predizer ação\n",
    "            state_tensor = tf.expand_dims(tf.convert_to_tensor(obs, dtype=tf.float32), 0)\n",
    "            logits = model(state_tensor)\n",
    "            prob_right = tf.nn.sigmoid(logits)\n",
    "            action_prob = prob_right.numpy()[0, 0]\n",
    "            action = 1 if np.random.random() < action_prob else 0\n",
    "            \n",
    "            # Executar ação\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            # Armazenar dados\n",
    "            states.append(obs)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            obs = next_obs\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        all_rewards.append(rewards)\n",
    "        all_states.append(states)\n",
    "        all_actions.append(actions)\n",
    "    \n",
    "    # Calcular vantagens\n",
    "    all_advantages = discount_and_normalize_rewards(all_rewards, discount_rate)\n",
    "    \n",
    "    # Preparar dados\n",
    "    all_states_flat = np.concatenate(all_states)\n",
    "    all_actions_flat = np.concatenate(all_actions)\n",
    "    all_advantages_flat = np.concatenate(all_advantages)\n",
    "    \n",
    "    # Treinamento\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(all_states_flat, all_actions_flat, all_advantages_flat)\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    # Log e save\n",
    "    if iteration % save_iterations == 0:\n",
    "        avg_reward = np.mean([sum(rewards) for rewards in all_rewards])\n",
    "        print(f\"Iteration {iteration}: Avg Reward = {avg_reward:.2f}, Loss = {loss:.4f}\")\n",
    "        \n",
    "        # Salvar modelo no formato moderno (.keras)\n",
    "        model_path = checkpoint_dir / f\"policy_net_iter_{iteration}.keras\"\n",
    "        model.save(model_path)\n",
    "        print(f\"Model saved as {model_path}\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"🎉 Treinamento concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01937fc8",
   "metadata": {},
   "source": [
    "Aprendizado de diferenças temporais e Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b43bfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Learning rate = 0.0500\n",
      "Iteration 1000: Learning rate = 0.0005\n",
      "Iteration 2000: Learning rate = 0.0002\n",
      "Iteration 3000: Learning rate = 0.0002\n",
      "Iteration 4000: Learning rate = 0.0001\n",
      "Iteration 5000: Learning rate = 0.0001\n",
      "Iteration 6000: Learning rate = 0.0001\n",
      "Iteration 7000: Learning rate = 0.0001\n",
      "Iteration 8000: Learning rate = 0.0001\n",
      "Iteration 9000: Learning rate = 0.0001\n",
      "Iteration 10000: Learning rate = 0.0000\n",
      "Iteration 11000: Learning rate = 0.0000\n",
      "Iteration 12000: Learning rate = 0.0000\n",
      "Iteration 13000: Learning rate = 0.0000\n",
      "Iteration 14000: Learning rate = 0.0000\n",
      "Iteration 15000: Learning rate = 0.0000\n",
      "Iteration 16000: Learning rate = 0.0000\n",
      "Iteration 17000: Learning rate = 0.0000\n",
      "Iteration 18000: Learning rate = 0.0000\n",
      "Iteration 19000: Learning rate = 0.0000\n",
      "\n",
      "🎯 Q-table final:\n",
      "Estado | Ação 0 | Ação 1 | Ação 2\n",
      "-----------------------------------\n",
      "     0 |  0.397 |  0.042 |  0.019\n",
      "     1 | -0.026 | -0.003 |  0.300\n",
      "     2 | -0.042 |  0.293 | -0.043\n",
      "\n",
      "🧠 Política ótima:\n",
      "Estado 0: Melhor ação = 0, Valor = 0.397\n",
      "Estado 1: Melhor ação = 2, Valor = 0.300\n",
      "Estado 2: Melhor ação = 1, Valor = 0.293\n"
     ]
    }
   ],
   "source": [
    "# Parâmetros do MDP\n",
    "n_states = 3\n",
    "n_actions = 3\n",
    "discount_rate = 0.95\n",
    "\n",
    "# Learning parameters\n",
    "learning_rate0 = 0.05  # Taxa de aprendizado inicial\n",
    "learning_rate_decay = 0.1\n",
    "n_iterations = 20000\n",
    "\n",
    "# Definir possible_actions (ações possíveis por estado)\n",
    "# Exemplo: cada estado tem todas as ações disponíveis\n",
    "possible_actions = [list(range(n_actions)) for _ in range(n_states)]\n",
    "\n",
    "# Matriz de transições T[s, a, s'] = P(s'|s, a)\n",
    "# Exemplo: transições aleatórias para demonstração\n",
    "T = np.random.dirichlet(np.ones(n_states), size=(n_states, n_actions))\n",
    "\n",
    "# Matriz de recompensas R[s, a, s'] = recompensa\n",
    "# Exemplo: recompensas aleatórias entre -1 e 1\n",
    "R = np.random.uniform(-1, 1, size=(n_states, n_actions, n_states))\n",
    "\n",
    "# Inicializar Q-table com -inf para ações não possíveis\n",
    "Q = np.full((n_states, n_actions), -np.inf)\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q[state, actions] = 0.0  # Inicializar ações possíveis com 0\n",
    "\n",
    "# Q-Learning\n",
    "s = 0  # Estado inicial\n",
    "history = []  # Para acompanhar a convergência\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    # Escolher ação ε-greedy (exploração vs exploração)\n",
    "    if np.random.random() < 0.1:  # 10% exploration\n",
    "        a = np.random.choice(possible_actions[s])\n",
    "    else:  # 90% exploitation\n",
    "        a = np.argmax(Q[s])\n",
    "    \n",
    "    # Simular transição de estado\n",
    "    sp = np.random.choice(range(n_states), p=T[s, a])\n",
    "    reward = R[s, a, sp]\n",
    "    \n",
    "    # Decaimento da taxa de aprendizado\n",
    "    learning_rate = learning_rate0 / (1 + iteration * learning_rate_decay)\n",
    "    \n",
    "    # Atualização Q-learning\n",
    "    Q[s, a] = ((1 - learning_rate) * Q[s, a] + \n",
    "               learning_rate * (reward + discount_rate * np.max(Q[sp])))\n",
    "    \n",
    "    # Registrar para análise\n",
    "    if iteration % 1000 == 0:\n",
    "        history.append(Q.copy())\n",
    "        print(f\"Iteration {iteration}: Learning rate = {learning_rate:.4f}\")\n",
    "    \n",
    "    # Mudar para próximo estado\n",
    "    s = sp\n",
    "\n",
    "# Resultados finais\n",
    "print(\"\\n🎯 Q-table final:\")\n",
    "print(\"Estado | Ação 0 | Ação 1 | Ação 2\")\n",
    "print(\"-\" * 35)\n",
    "for state in range(n_states):\n",
    "    print(f\"{state:6} | {Q[state, 0]:6.3f} | {Q[state, 1]:6.3f} | {Q[state, 2]:6.3f}\")\n",
    "\n",
    "print(\"\\n🧠 Política ótima:\")\n",
    "for state in range(n_states):\n",
    "    best_action = np.argmax(Q[state])\n",
    "    print(f\"Estado {state}: Melhor ação = {best_action}, Valor = {Q[state, best_action]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90f89b5",
   "metadata": {},
   "source": [
    "#### Exercícios:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695fe4ce",
   "metadata": {},
   "source": [
    "1. Como você definiria o Aprendizado por reforço? Como ele é diferente do aprendizado regular supervisionado ou não supervisionado?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641febef",
   "metadata": {},
   "source": [
    "**Aprendizado por Reforço** é um tipo de aprendizado de máquina onde um agente aprende a tomar decisões através de tentativa e erro, recebendo recompensas ou punições como feedback. Diferente do aprendizado supervisionado (que usa exemplos pré-definidos com respostas corretas) e do não supervisionado (que busca padrões sem feedback), o aprendizado por reforço aprende interagindo com o ambiente e maximizando recompensas acumuladas ao longo do tempo.\n",
    "\n",
    "Enquanto o aprendizado supervisionado precisa de um \"professor\" fornecendo respostas exatas, e o não supervisionado descobre estrutura sozinho, o aprendizado por reforço aprende pela experiência, como um jogador que melhora praticando e recebendo pontos por suas ações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf6586",
   "metadata": {},
   "source": [
    "2. Você consegue pensar em três possíveis aplicações do RL que não foram mencionadas neste capítulo? Qual é o ambiente para cada uma delas? Qual é o agente? quais são as ações possíveis? Quais são as recompensas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a70adce",
   "metadata": {},
   "source": [
    "**A. Otimização de Tráfego em Tempo Real**\n",
    "- **Ambiente**: Rede de semáforos de uma cidade\n",
    "- **Agente**: Sistema de controle de tráfego\n",
    "- **Ações**: Alterar tempos dos semáforos, priorizar vias específicas\n",
    "- **Recompensas**: Redução do tempo médio de espera, aumento do fluxo de veículos\n",
    "\n",
    "**B. Gestão Automatizada de Portfólio Financeiro**\n",
    "- **Ambiente**: Mercado financeiro com ações, títulos e commodities\n",
    "- **Agente**: Sistema de investimentos automatizado\n",
    "- **Ações**: Comprar, vender ou manter ativos financeiros\n",
    "- **Recompensas**: Maximizar retorno do portfólio, minimizar riscos\n",
    "\n",
    "**C. Controle de Eficiência Energética em Data Centers**\n",
    "- **Ambiente**: Sistema de refrigeração e servidores do data center\n",
    "- **Agente**: Controlador de gestão energética\n",
    "- **Ações**: Ajustar temperaturas, redistribuir cargas, gerenciar cooling\n",
    "- **Recompensas**: Redução do consumo energético, manutenção da temperatura ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d581769",
   "metadata": {},
   "source": [
    "3. Qual é a taxa de desconto? A política ótima pode mudar se você modificar a taxa de desconto?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9365a8e",
   "metadata": {},
   "source": [
    "A **taxa de desconto** (γ) é um hiperparâmetro entre 0 e 1 que determina o valor presente de recompensas futuras. Valores próximos a 1 tornam o agente mais orientado a longo prazo, enquanto valores próximos a 0 focam em recompensas imediatas.\n",
    "\n",
    "Sim, a **política ótima pode mudar** ao alterar a taxa de desconto. Uma taxa baixa prioriza recompensas imediatas, levando a políticas mais \"curtas\" e oportunistas. Já uma taxa alta valoriza consequências futuras, resultando em políticas mais estratégicas e de longo prazo. A escolha da taxa reflete o trade-off entre satisfação imediata e planejamento futuro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3c317a",
   "metadata": {},
   "source": [
    "4. Como você mede o desempenho de um agente do aprendizado por reforço?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026ce8e0",
   "metadata": {},
   "source": [
    "O desempenho de um agente de aprendizado por reforço é medido principalmente pela **soma acumulada de recompensas** obtida durante os episódios de treinamento e teste. Outras métricas importantes incluem a **taxa de sucesso** (quantas vezes atingiu o objetivo), o **tempo de convergência** (quantos episódios até estabilizar o desempenho) e a **consistência** (variação do desempenho entre execuções). Em ambientes contínuos, também se avalia a **eficiência** (custo computacional por ação) e a **capacidade de generalização** para situações não vistas durante o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79673e1",
   "metadata": {},
   "source": [
    "5. Qual é o problema da atribuição de crédito? Quando ocorre? Como pode ser aliviado?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c612f2",
   "metadata": {},
   "source": [
    "O **problema da atribuição de crédito** ocorre quando um agente recebe uma recompensa após uma sequência de ações e precisa determinar quais ações específicas contribuíram para esse resultado. Este problema é comum em tarefas com recompensas esparsas ou atrasadas, onde o feedback não é imediato.\n",
    "\n",
    "Pode ser aliviado através de métodos como **Discounting** (valorizando recompensas imediatas), **Eligibility Traces** (rastreando contribuições de ações recentes) e **Reward Shaping** (fornecendo recompensas intermediárias artificiais para guiar o aprendizado). Algoritmos como **Actor-Critic** também ajudam ao separar a avaliação de valores da seleção de ações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df70696",
   "metadata": {},
   "source": [
    "6. Qual é o objetivo de utilizar uma memória de repetição?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b7ca6",
   "metadata": {},
   "source": [
    "A **memória de repetição** (replay buffer) é utilizada para armazenar experiências passadas do agente (estado, ação, recompensa, próximo estado), permitindo o reaproveitamento dessas experiências em treinamentos futuros. Seu objetivo principal é **quebrar correlações temporais** entre experiências sequenciais, aumentar a **eficiência de amostragem** e **estabilizar o treinamento** ao permitir que o agente aprenda repetidamente com experiências diversificadas, incluindo tanto sucessos quanto fracassos passados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a23ea7",
   "metadata": {},
   "source": [
    "7. O que é um algoritmo de RL off-policy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f8f3dc",
   "metadata": {},
   "source": [
    "Um algoritmo **off-policy** é um método de aprendizado por reforço onde o agente aprende sobre uma política ótima (política alvo) enquanto segue uma política diferente de exploração (política comportamental). Isso permite aprender com experiências passadas geradas por políticas diferentes, reutilizar dados de forma mais eficiente e explorar o ambiente sem comprometer o aprendizado da política ideal. Exemplos comuns incluem Q-Learning e DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fc5b88",
   "metadata": {},
   "source": [
    "8. Utilize gradientes de políticas para enfrentar o \"BypedalWalker-v2\" da OpenAI gym."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52687414",
   "metadata": {},
   "source": [
    "9. Utilize o algoritmo DQN para treinar um agente para jogar Pong, o famoso jogo do atari(Pong-v0 no OpenAI gym). Cuidado: uma observação individual é insuficiente para dizer a direção e a velocidade da bola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9718e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
